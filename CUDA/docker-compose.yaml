version: "3.9"
services:
    base:
        entrypoint: ["/bin/bash", "-c", "trap : TERM INT; sleep infinity & wait"]
        image: nvcr.io/nvidia/pytorch:24.07-py3 #rocm/pytorch:rocm6.1_ubuntu22.04_py3.10_pytorch_2.4
        hostname: test
        build:
          context: .
          dockerfile: Dockerfile
        environment:
            - USE_HTTPS=0
            - INSTALL_ROOT=${INSTALL_ROOT}
        ports:
            - "8899:8899"
        volumes:
            # - type: bind
            #   source: models
            #   target: ${INSTALL_ROOT}/models
            - type: bind
              source: include
              target: ${INSTALL_ROOT}/include
            - type: bind
              source: lib
              target: ${INSTALL_ROOT}/lib
            - type: bind
              source: python
              target: ${INSTALL_ROOT}/python
            - type: bind
              source: scripts
              target: ${INSTALL_ROOT}/scripts
            - type: bind
              source: src
              target: ${INSTALL_ROOT}/src
            - type: bind
              source: tests
              target: ${INSTALL_ROOT}/tests
            #   read_only: true
        deploy:
          resources:
            reservations:
              devices:
                - driver: nvidia
                  count: 1
                  capabilities: [gpu]

        # devices:
        #     - /dev/kfd
        #     - /dev/dri
        # group_add:
        #     - video
        # ipc: host
        # cap_add:
        #     - SYS_PTRACE
        # security_opt:
        #     - seccomp=unconfined

networks:
  llama-network:
    name: llama_backend_network


